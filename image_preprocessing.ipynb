{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gesture Recognition Preprocessing\n",
        "\n",
        "1. Load the CSV files that describe each video.\n",
        "2. Check that every video folder and its images exist.\n",
        "3. Split videos into train / validation / test sets.\n",
        "4. Resize and normalize images, and save them in a clean format.\n",
        "5. Balance the training set by creating extra (augmented) videos for minority classes.\n",
        "\n",
        "The goal is a **clean, balanced, ready‑to‑train dataset** in a directory called `processed_dataset/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 0: Imports\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Step 1: Define constants and paths\n",
        "DATASET_DIR = \"dataset\"\n",
        "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
        "TRAIN_CSV = os.path.join(DATASET_DIR, \"train.csv\")\n",
        "\n",
        "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
        "VAL_CSV = os.path.join(DATASET_DIR, \"val.csv\")\n",
        "\n",
        "OUTPUT_DIR = \"processed_dataset\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# Image dimensions\n",
        "IMG_WIDTH = 64\n",
        "IMG_HEIGHT = 64\n",
        "\n",
        "# Train / validation / test split ratios\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train CSV shape: (663, 3)\n",
            "Val CSV shape: (100, 3)\n",
            "                                   video_id         gesture  label\n",
            "0  WIN_20180925_17_08_43_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
            "1  WIN_20180925_17_18_28_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
            "2  WIN_20180925_17_18_56_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
            "3  WIN_20180925_17_19_51_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
            "4  WIN_20180925_17_20_14_Pro_Left_Swipe_new  Left_Swipe_new      0\n",
            "\n",
            "Total number of videos (rows): 763\n",
            "Canonical class names (from numeric labels): ['Left_Swipe' 'Right_Swipe' 'Stop' 'Thumbs_Down' 'Thumbs_Up']\n",
            "Unique numeric labels: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load CSV files and look at them\n",
        "\n",
        "# video_folder_name;    gesture_name;   numeric_label\n",
        "column_names = [\"video_id\", \"gesture\", \"label\"]\n",
        "\n",
        "train_csv = pd.read_csv(TRAIN_CSV, sep=\";\", header=None, names=column_names)\n",
        "val_csv = pd.read_csv(VAL_CSV, sep=\";\", header=None, names=column_names)\n",
        "\n",
        "print(\"Train CSV shape:\", train_csv.shape)\n",
        "print(\"Val CSV shape:\", val_csv.shape)\n",
        "\n",
        "print(train_csv.head())\n",
        "\n",
        "# Combine for easier processing; remember original source split\n",
        "train_csv[\"source_split\"] = \"train\"\n",
        "val_csv[\"source_split\"] = \"val\"\n",
        "all_videos = pd.concat([train_csv, val_csv], ignore_index=True)\n",
        "\n",
        "# Map numeric labels (0–4) to exactly 5 canonical class names\n",
        "label_to_class = {\n",
        "    0: \"Left_Swipe\",\n",
        "    1: \"Right_Swipe\",\n",
        "    2: \"Stop\",\n",
        "    3: \"Thumbs_Down\",\n",
        "    4: \"Thumbs_Up\",\n",
        "}\n",
        "\n",
        "# Add a clean class_name column based only on the numeric label\n",
        "all_videos[\"class_name\"] = all_videos[\"label\"].map(label_to_class)\n",
        "\n",
        "# Also overwrite the gesture column so later code uses only these 5 names\n",
        "all_videos[\"gesture\"] = all_videos[\"class_name\"]\n",
        "\n",
        "print(\"\\nTotal number of videos (rows):\", len(all_videos))\n",
        "print(\"Canonical class names (from numeric labels):\", all_videos[\"class_name\"].unique())\n",
        "print(\"Unique numeric labels:\", sorted(all_videos[\"label\"].unique()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos: 763\n",
            "Videos with existing folders: 763\n",
            "\n",
            "Frame count statistics (before dropping empties):\n",
            "count    763.0\n",
            "mean      30.0\n",
            "std        0.0\n",
            "min       30.0\n",
            "25%       30.0\n",
            "50%       30.0\n",
            "75%       30.0\n",
            "max       30.0\n",
            "Name: num_frames, dtype: float64\n",
            "\n",
            "Remaining videos after cleaning: 763\n",
            "Class distribution (labels):\n",
            "label\n",
            "0    154\n",
            "1    160\n",
            "2    152\n",
            "3    158\n",
            "4    139\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Add folder paths and remove obviously bad entries\n",
        "\n",
        "# For each row, build the path to the video folder that contains its images\n",
        "video_paths = []\n",
        "\n",
        "for idx, row in all_videos.iterrows():\n",
        "    video_id = row[\"video_id\"]\n",
        "    if row[\"source_split\"] == \"train\":\n",
        "        folder = os.path.join(TRAIN_DIR, video_id)\n",
        "    else:\n",
        "        folder = os.path.join(VAL_DIR, video_id)\n",
        "    video_paths.append(folder)\n",
        "\n",
        "all_videos[\"video_folder\"] = video_paths\n",
        "\n",
        "# Keep only rows where the folder actually exists\n",
        "exists_mask = all_videos[\"video_folder\"].apply(os.path.isdir)\n",
        "clean_meta = all_videos[exists_mask].copy()\n",
        "\n",
        "print(\"Total videos:\", len(all_videos))\n",
        "print(\"Videos with existing folders:\", len(clean_meta))\n",
        "\n",
        "# count how many PNG images each folder has; drop folders with 0 images\n",
        "num_frames = []\n",
        "\n",
        "for folder in clean_meta[\"video_folder\"]:\n",
        "    images = [f for f in os.listdir(folder) if f.lower().endswith(\".png\")]\n",
        "    num_frames.append(len(images))\n",
        "\n",
        "clean_meta[\"num_frames\"] = num_frames\n",
        "\n",
        "print(\"\\nFrame count statistics (before dropping empties):\")\n",
        "print(clean_meta[\"num_frames\"].describe())\n",
        "\n",
        "# Remove videos that have no frames\n",
        "clean_meta = clean_meta[clean_meta[\"num_frames\"] > 0].reset_index(drop=True)\n",
        "\n",
        "print(\"\\nRemaining videos after cleaning:\", len(clean_meta))\n",
        "print(\"Class distribution (labels):\")\n",
        "print(clean_meta[\"label\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train videos: 533\n",
            "Val videos: 115\n",
            "Test videos: 115\n",
            "\n",
            "Train label counts:\n",
            "label\n",
            "0    108\n",
            "1    112\n",
            "2    106\n",
            "3    110\n",
            "4     97\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Val label counts:\n",
            "label\n",
            "0    23\n",
            "1    24\n",
            "2    23\n",
            "3    24\n",
            "4    21\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test label counts:\n",
            "label\n",
            "0    23\n",
            "1    24\n",
            "2    23\n",
            "3    24\n",
            "4    21\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create train / validation / test splits (video level)\n",
        "\n",
        "labels = clean_meta[\"label\"]\n",
        "\n",
        "# First split: train+val vs test\n",
        "train_val_meta, test_meta = train_test_split(\n",
        "    clean_meta,\n",
        "    test_size=TEST_RATIO,\n",
        "    stratify=labels,\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "# Second split: train vs val (from the train_val set)\n",
        "val_ratio_adjusted = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
        "\n",
        "train_meta, val_meta = train_test_split(\n",
        "    train_val_meta,\n",
        "    test_size=val_ratio_adjusted,\n",
        "    stratify=train_val_meta[\"label\"],\n",
        "    random_state=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "print(\"Train videos:\", len(train_meta))\n",
        "print(\"Val videos:\", len(val_meta))\n",
        "print(\"Test videos:\", len(test_meta))\n",
        "\n",
        "print(\"\\nTrain label counts:\")\n",
        "print(train_meta[\"label\"].value_counts().sort_index())\n",
        "print(\"\\nVal label counts:\")\n",
        "print(val_meta[\"label\"].value_counts().sort_index())\n",
        "print(\"\\nTest label counts:\")\n",
        "print(test_meta[\"label\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: load, resize, and normalize a whole video\n",
        "\n",
        "# We will save each processed video as a NumPy array file (.npy)\n",
        "# with shape: (num_frames, IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "# Pixel values will be in the range [0, 1].\n",
        "\n",
        "\n",
        "def load_and_process_video(folder_path):\n",
        "    \"\"\"Load all PNG images from a folder, resize them, normalize them, and\n",
        "    return them as a NumPy array. If any image is corrupted, return None.\n",
        "    \"\"\"\n",
        "    file_names = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".png\")])\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    for name in file_names:\n",
        "        file_path = os.path.join(folder_path, name)\n",
        "        try:\n",
        "            img = Image.open(file_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(\"Could not open image, skipping whole video:\", file_path, \"Error:\", e)\n",
        "            return None\n",
        "\n",
        "        # Resize image\n",
        "        img = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "        # Convert to NumPy array and normalize to [0, 1]\n",
        "        arr = np.array(img, dtype=\"float32\") / 255.0\n",
        "        frames.append(arr)\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return None\n",
        "\n",
        "    video_array = np.stack(frames, axis=0)  # shape: (num_frames, H, W, 3)\n",
        "    return video_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing split: train\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished train split.\n",
            "Good videos: 533\n",
            "Skipped (corrupted / empty) videos: 0\n",
            "\n",
            "Processing split: val\n",
            "Finished val split.\n",
            "Good videos: 115\n",
            "Skipped (corrupted / empty) videos: 0\n",
            "\n",
            "Processing split: test\n",
            "Finished test split.\n",
            "Good videos: 115\n",
            "Skipped (corrupted / empty) videos: 0\n",
            "\n",
            "Total processed videos (before augmentation): 763\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Process videos \n",
        "\n",
        "processed_records = [] \n",
        "\n",
        "# Create split folders inside OUTPUT_DIR\n",
        "for split_name in [\"train\", \"val\", \"test\"]:\n",
        "    split_folder = os.path.join(OUTPUT_DIR, split_name)\n",
        "    if not os.path.exists(split_folder):\n",
        "        os.makedirs(split_folder)\n",
        "\n",
        "\n",
        "def process_split(split_name, meta_df):\n",
        "    print(\"\\nProcessing split:\", split_name)\n",
        "    count_ok = 0\n",
        "    count_bad = 0\n",
        "\n",
        "    for _, row in meta_df.iterrows():\n",
        "        video_id = row[\"video_id\"]\n",
        "        gesture = row[\"gesture\"]\n",
        "        label = int(row[\"label\"])\n",
        "        folder = row[\"video_folder\"]\n",
        "\n",
        "        # Build output folder and file path\n",
        "        gesture_folder = os.path.join(OUTPUT_DIR, split_name, gesture)\n",
        "        if not os.path.exists(gesture_folder):\n",
        "            os.makedirs(gesture_folder)\n",
        "\n",
        "        out_file_path = os.path.join(gesture_folder, video_id + \".npy\")\n",
        "\n",
        "        # Load and process\n",
        "        video_array = load_and_process_video(folder)\n",
        "\n",
        "        if video_array is None:\n",
        "            count_bad += 1\n",
        "            continue\n",
        "\n",
        "        # Save as .npy\n",
        "        np.save(out_file_path, video_array)\n",
        "        count_ok += 1\n",
        "\n",
        "        # Add to metadata list\n",
        "        processed_records.append(\n",
        "            {\n",
        "                \"split\": split_name,\n",
        "                \"video_id\": video_id,\n",
        "                \"gesture\": gesture,\n",
        "                \"label\": label,\n",
        "                \"file_path\": out_file_path,\n",
        "                \"num_frames\": video_array.shape[0],\n",
        "                \"is_augmented\": False,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"Finished\", split_name, \"split.\")\n",
        "    print(\"Good videos:\", count_ok)\n",
        "    print(\"Skipped (corrupted / empty) videos:\", count_bad)\n",
        "\n",
        "\n",
        "process_split(\"train\", train_meta)\n",
        "process_split(\"val\", val_meta)\n",
        "process_split(\"test\", test_meta)\n",
        "\n",
        "print(\"\\nTotal processed videos (before augmentation):\", len(processed_records))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Per-class counts:\n",
            "label\n",
            "0    108\n",
            "1    112\n",
            "2    106\n",
            "3    110\n",
            "4     97\n",
            "Name: count, dtype: int64\n",
            "Target count for each class: 112\n",
            "\n",
            "Augmenting label 0 (need 4 more videos)\n",
            "\n",
            "Augmenting label 2 (need 6 more videos)\n",
            "\n",
            "Augmenting label 3 (need 2 more videos)\n",
            "\n",
            "Augmenting label 4 (need 15 more videos)\n",
            "\n",
            "Number of new augmented train videos: 27\n",
            "\n",
            "Train set label counts AFTER augmentation:\n",
            "label\n",
            "0    112\n",
            "1    112\n",
            "2    112\n",
            "3    112\n",
            "4    112\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Balance the training set using simple data augmentation\n",
        "\n",
        "# We will create extra videos for classes that have fewer samples.\n",
        "# Augmentation: for each new video, we will take an existing one and\n",
        "# horizontally flip every frame.\n",
        "\n",
        "meta_df = pd.DataFrame(processed_records)\n",
        "\n",
        "train_meta_df = meta_df[meta_df[\"split\"] == \"train\"].copy()\n",
        "\n",
        "# Compute how many samples each class has\n",
        "label_counts = train_meta_df[\"label\"].value_counts()\n",
        "max_count = label_counts.max()\n",
        "\n",
        "print(\"\\nPer-class counts:\")\n",
        "print(label_counts.sort_index())\n",
        "print(\"Target count for each class:\", max_count)\n",
        "\n",
        "new_augmented_records = []\n",
        "\n",
        "for label_value in sorted(label_counts.index):\n",
        "    current_count = label_counts[label_value]\n",
        "    needed = max_count - current_count\n",
        "\n",
        "    if needed <= 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nAugmenting label {label_value} (need {needed} more videos)\")\n",
        "\n",
        "    # All videos from this class in the train split\n",
        "    class_videos = train_meta_df[train_meta_df[\"label\"] == label_value].reset_index(drop=True)\n",
        "\n",
        "    for i in range(needed):\n",
        "        # Pick a random existing video from this class\n",
        "        source_row = class_videos.sample(n=1, random_state=random.randint(0, 1_000_000)).iloc[0]\n",
        "\n",
        "        source_path = source_row[\"file_path\"]\n",
        "        source_video_id = source_row[\"video_id\"]\n",
        "        gesture = source_row[\"gesture\"]\n",
        "\n",
        "        # Load the stored NumPy array\n",
        "        video_array = np.load(source_path)\n",
        "\n",
        "        # Augmentation: horizontal flip (reverse width axis)\n",
        "        # video_array shape: (num_frames, H, W, 3)\n",
        "        flipped = video_array[:, :, ::-1, :]\n",
        "\n",
        "        # New video id and path\n",
        "        new_video_id = f\"{source_video_id}_aug{i}\"\n",
        "        new_file_path = os.path.join(OUTPUT_DIR, \"train\", gesture, new_video_id + \".npy\")\n",
        "\n",
        "        # Save augmented video\n",
        "        np.save(new_file_path, flipped)\n",
        "\n",
        "        # Record metadata\n",
        "        new_augmented_records.append(\n",
        "            {\n",
        "                \"split\": \"train\",\n",
        "                \"video_id\": new_video_id,\n",
        "                \"gesture\": gesture,\n",
        "                \"label\": int(label_value),\n",
        "                \"file_path\": new_file_path,\n",
        "                \"num_frames\": flipped.shape[0],\n",
        "                \"is_augmented\": True,\n",
        "                \"source_video_id\": source_video_id,\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(\"\\nNumber of new augmented train videos:\", len(new_augmented_records))\n",
        "\n",
        "# Update full metadata\n",
        "if new_augmented_records:\n",
        "    meta_df = pd.concat([meta_df, pd.DataFrame(new_augmented_records)], ignore_index=True)\n",
        "\n",
        "train_meta_df = meta_df[meta_df[\"split\"] == \"train\"].copy()\n",
        "\n",
        "print(\"\\nTrain set label counts AFTER augmentation:\")\n",
        "print(train_meta_df[\"label\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metadata to: processed_dataset\\videos_metadata.csv\n"
          ]
        }
      ],
      "source": [
        "# Save full metadata for all splits\n",
        "meta_csv_path = os.path.join(OUTPUT_DIR, \"videos_metadata.csv\")\n",
        "meta_df.to_csv(meta_csv_path, index=False)\n",
        "\n",
        "print(\"Saved metadata to:\", meta_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we:\n",
        "\n",
        "- **Loaded** the gesture recognition CSV files and built a simple table of all videos.\n",
        "- **Checked folders and frames**, removing videos that were missing or empty.\n",
        "- **Split** the data into train, validation, and test sets at the video level.\n",
        "- **Resized and normalized** all images, saving each video as a `.npy` file with pixel values in \\([0, 1]\\).\n",
        "- **Balanced the training set** by adding horizontally‑flipped copies of videos from smaller classes.\n",
        "- **Saved metadata** about every processed video in `processed_dataset/videos_metadata.csv`.\n",
        "\n",
        "You can now load the `.npy` files and metadata in your training notebook and focus on building your model.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
